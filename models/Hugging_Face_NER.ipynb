{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvkxSgwyRp9H",
        "outputId": "05b63f20-fff9-4675-d038-f893a143adf1"
      },
      "outputs": [],
      "source": [
        "! pip install transformers datasets tokenizers evaluate\n",
        "! pip install transformers[sentencepiece]\n",
        "! pip install torch\n",
        "! pip install tensorflow\n",
        "! pip install spacy\n",
        "! pip install seqeval\n",
        "! pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install ipywidgets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIRIdzTjZuOx"
      },
      "outputs": [],
      "source": [
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from transformers import pipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\",ignore_mismatched_sizes=True)\n",
        "\n",
        "nlp = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
        "example = \"Who is David Cooperfield? He is a magician from New York.\"\n",
        "\n",
        "ner_results = nlp(example)\n",
        "print(ner_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Need to process scenarios data to CONLL-2003 format\n",
        "# 2. Load the data to Dataset class\n",
        "# If the model only need token and ner-tag, then it's easy; otherwise, we need POS tagging, chunk tagging, which need another model to do this job.\n",
        "import sys\n",
        "sys.path.insert(1, 'D:\\GradProject\\mobile_privacy\\mobile_privacy')\n",
        "import datasets\n",
        "from notebooks.lib_analysis import *\n",
        "\n",
        "def transform_ner_tags_to_conll2003_format(ner_tags):\n",
        "    '''\n",
        "    Transform the ner_tags to CONLL-2003 format.\n",
        "    '''\n",
        "    res = []\n",
        "    for tag in ner_tags:\n",
        "        if tag == 'o':\n",
        "            res.append('O')\n",
        "        elif tag == 'b-i':\n",
        "            res.append('B-MISC')\n",
        "        elif tag == 'i-i':\n",
        "            res.append('I-MISC')\n",
        "    return res\n",
        "\n",
        "def load_dataset(path):\n",
        "    '''\n",
        "    Load the dataset from the path as a Dataset object.\n",
        "    Dataset format is:\n",
        "        - id: the id of the scenario\n",
        "        - tokens: tokennized words\n",
        "        - ner_tags: the NER tags of the tokens\n",
        "    '''\n",
        "    data1 = read_and_parse_data(path)\n",
        "    res = {'id': [], 'tokens': [], 'ner_tags': []}\n",
        "    for id, data in data1.items():\n",
        "        res['id'].append(id)\n",
        "        res['tokens'].append(data['words'])\n",
        "        res['ner_tags'].append(transform_ner_tags_to_conll2003_format(data['codes']))\n",
        "    return datasets.Dataset.from_dict(res, features=datasets.Features(\n",
        "                {\n",
        "                    \"id\": datasets.Value(\"string\"),\n",
        "                    \"tokens\": datasets.Sequence(datasets.Value(\"string\")),\n",
        "                    \"ner_tags\": datasets.Sequence(\n",
        "                        datasets.features.ClassLabel(\n",
        "                            names=[\n",
        "                                'O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC'\n",
        "                            ]\n",
        "                        )\n",
        "                    ),\n",
        "                }\n",
        "            ))\n",
        "\n",
        "samples = load_dataset('sample1_3.txt')\n",
        "scenarios = load_dataset('scenario1_2.txt')\n",
        "raw_datasets = datasets.dataset_dict.DatasetDict({'train': scenarios, 'validation': samples})\n",
        "ner_feature = raw_datasets[\"train\"].features[\"ner_tags\"]\n",
        "label_names = ner_feature.feature.names\n",
        "\n",
        "# auto tokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_checkpoint = \"dslim/bert-base-NER\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "inputs = tokenizer(raw_datasets[\"train\"][0][\"tokens\"], is_split_into_words=True)\n",
        "\n",
        "# tokenize and align dataset\n",
        "def align_labels_with_tokens(labels, word_ids):\n",
        "    new_labels = []\n",
        "    current_word = None\n",
        "    for word_id in word_ids:\n",
        "        if word_id != current_word:\n",
        "            # Start of a new word!\n",
        "            current_word = word_id\n",
        "            label = -100 if word_id is None else labels[word_id]\n",
        "            new_labels.append(label)\n",
        "        elif word_id is None:\n",
        "            # Special token\n",
        "            new_labels.append(-100)\n",
        "        else:\n",
        "            # Same word as previous token\n",
        "            label = labels[word_id]\n",
        "            # If the label is B-XXX we change it to I-XXX\n",
        "            if label % 2 == 1:\n",
        "                label += 1\n",
        "            new_labels.append(label)\n",
        "\n",
        "    return new_labels\n",
        "\n",
        "def tokenize_and_align_labels(examples):\n",
        "    tokenized_inputs = tokenizer(\n",
        "        examples[\"tokens\"], truncation=True, is_split_into_words=True\n",
        "    )\n",
        "    all_labels = examples[\"ner_tags\"]\n",
        "    new_labels = []\n",
        "    for i, labels in enumerate(all_labels):\n",
        "        word_ids = tokenized_inputs.word_ids(i)\n",
        "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = new_labels\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_datasets = raw_datasets.map(\n",
        "    tokenize_and_align_labels,\n",
        "    batched=True,\n",
        "    remove_columns=raw_datasets[\"train\"].column_names,\n",
        ")\n",
        "\n",
        "# data collation\n",
        "from transformers import DataCollatorForTokenClassification\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "# metric\n",
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"seqeval\")\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "\n",
        "    # Remove ignored index (special tokens) and convert to labels\n",
        "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
        "    true_predictions = [\n",
        "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
        "        for prediction, label in zip(predictions, labels)\n",
        "    ]\n",
        "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
        "    return {\n",
        "        \"precision\": all_metrics[\"overall_precision\"],\n",
        "        \"recall\": all_metrics[\"overall_recall\"],\n",
        "        \"f1\": all_metrics[\"overall_f1\"],\n",
        "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
        "    }\n",
        "\n",
        "# define model\n",
        "id2label = {i: label for i, label in enumerate(label_names)}\n",
        "label2id = {v: k for k, v in id2label.items()}\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    \"bert-finetuned-ner\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    num_train_epochs=30,\n",
        "    weight_decay=0.01,\n",
        "    push_to_hub=False,\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "data1 = read_and_parse_data('../datasets/sample1-TH.json')\n",
        "data = {'id': [], 'words': [], 'codes': []}\n",
        "for id, text, clean_text, words, codes, scores in data1.items():\n",
        "    data['id'].append(id)\n",
        "    data['words'].append(words)\n",
        "    data['codes'].append(codes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Process the data, to create a raw dataset that fits the format of CONLL-2003\n",
        "# 2. Create a tokenized dataset, that contains the tokenized version of the raw dataset\n",
        "# 3. Collate the tokenized dataset with DataCollatorForTokenClassification\n",
        "# 4. Define a compute_metrics() function that takes the arrays of predictions and labels, and returns a dictionary with the metric names and values.\n",
        "# 5. Create a Trainer object, and train the model\n",
        "# 6. Evaluate the model on the test set\n",
        "# Detail on https://huggingface.co/course/chapter7/2"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "16ef625867ebbfc84221a1bddf3d92c3ee48085fc071cd8ff0fe847c6a756416"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
