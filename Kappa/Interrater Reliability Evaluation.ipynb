{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Cohen's Kappa for Interrater Reliability\n",
    "\n",
    "The coding frame development was conducted in three rounds by two raters, who coded three sets of ten scenarios during each round. (1) The first round only used a single code 'information' and yielded a Cohen's Kappa of 0.2971. After this round, the raters met and developed the initial coding frame, consisting of three sub-codes: simple, complex and question. (2) In the second round, the raters used the new coding frame to yield a Cohen's Kappa of 0.5746. After this round, the raters met to discuss differences and to refine their understanding of the sub-codes. (3) The third round evaluated their refined understanding to yield an above chance agreement of 0.7745."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario ID length mismatch: samples/sample1-rater1-reann.json, samples/sample1-rater2.json\n",
      "Cohen's Kappa Round 1: 0.5746\n",
      "Cohen's Kappa Round 2: 0.6941\n",
      "Cohen's Kappa Round 3: 0.7745\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import json\n",
    "\n",
    "files = [\n",
    "    ['samples/sample1-rater1.json', 'samples/sample1-rater2.json'],\n",
    "    ['samples/sample2-rater1.json', 'samples/sample2-rater2.json'],\n",
    "    ['samples/sample2-rater1-reann.json', 'samples/sample2-rater2-reann.json'],\n",
    "    ['samples/sample3-rater1.json', 'samples/sample3-rater2.json']\n",
    "]\n",
    "\n",
    "# for each round of annotations, compute Cohen's Kappa\n",
    "round = 1\n",
    "for f1, f2 in files:\n",
    "    data1 = json.load(open(f1, 'r'))\n",
    "    data2 = json.load(open(f2, 'r'))\n",
    "\n",
    "    # check that each dataset contains the same number of scenarios\n",
    "    scenario_ids1 = list(data1.keys())\n",
    "    scenario_ids2 = list(data2.keys())\n",
    "    if len(scenario_ids1) != len(scenario_ids2):\n",
    "        print('Scenario ID length mismatch: %s, %s' % (f1, f2))\n",
    "        continue\n",
    "    \n",
    "    # check that each dataset contains the same scenario IDs\n",
    "    d1 = set(scenario_ids1) - set(scenario_ids2)\n",
    "    if len(d1) > 0:\n",
    "        print('Missing IDs from %s: %s' % (f2, d1))\n",
    "        continue\n",
    "    d2 = set(scenario_ids2) - set(scenario_ids1)\n",
    "    if len(d2) > 0:\n",
    "        print('Missing IDs from %s: %s' % (f1, d2))\n",
    "        continue\n",
    "    \n",
    "    # compile an ordered list of codes for computing Kappa\n",
    "    all_codes1 = []\n",
    "    all_codes2 = []\n",
    "    for scenario_id in scenario_ids1:\n",
    "        all_codes1.extend(data1[scenario_id]['codes'])\n",
    "        all_codes2.extend(data2[scenario_id]['codes'])\n",
    "    \n",
    "    # verify the number of codes is the same between datasets\n",
    "    if len(all_codes1) != len(all_codes2):\n",
    "        print('Code length mismatch: %s, %s' % (f1, f2))\n",
    "        continue\n",
    "\n",
    "    # compute Kappa\n",
    "    kappa = cohen_kappa_score(all_codes1, all_codes2)\n",
    "    print('Cohen\\'s Kappa Round %i: %0.4f' % (round, kappa))\n",
    "    round += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
