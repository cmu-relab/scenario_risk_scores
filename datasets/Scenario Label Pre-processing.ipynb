{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Overview\n",
    "\n",
    "This notebook is used to translate the multi-labeled scenario text into training, testing and validation datasets for machine learning. This division would normally correspond to a 80/10/10 split, respectively.\n",
    "\n",
    "While training the BERT-based model using the HuggingFace API can be performed on word-token sequences, the evaluation for the BERT-based model assumes complete scenario texts. Therefore, we split the dataset using a random selection of scenarios and not using a random selection of sentences, which could provide more sample diversity in training. The CRF-based model requires sentence-token sequences, thus, each scenario's token sequence in the dataset is organized by sentence. The CRF-based model also requires part-of-speech (POS) as input to feature development, in addition to tokenized words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': \"From this screen, I like to search for anything from recipes, to home decor, to people, etc., just depending on my mood. To get to this screen all I had to do was tap on the little magnifying glass next to the image of the home icon, and this is where you would search for whatever you like, kind of like how it works with google, but more customized to your preferences and trends. To find whatever I'm looking for, I click on the search bar to type a word or phrase. I try to keep it brief. For example, if I'm looking for a pasta recipe for dinner, I'll type pasta dinner recipe, then select the phrase that matches what I'm looking for. I then rummage through the available pins and decide which one I like the most before saving the pin to the board that most closely matches the kind of query I made. \",\n",
       " 'scenario_id': 'MAS-G-0001',\n",
       " 'app_url': 'https://apps.apple.com/us/app/pinterest/id429047995',\n",
       " 'labels': [[53, 60, 'SIM'],\n",
       "  [65, 75, 'SIM'],\n",
       "  [80, 86, 'SIM'],\n",
       "  [359, 370, 'SIM'],\n",
       "  [375, 381, 'SIM'],\n",
       "  [432, 442, 'SIM'],\n",
       "  [453, 457, 'SIM'],\n",
       "  [461, 467, 'SIM'],\n",
       "  [533, 539, 'SIM'],\n",
       "  [575, 581, 'SIM']]}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# load the unprocessed, labeled scenarios\n",
    "scenarios = json.load(open('scenarios-labeled.json'))\n",
    "scenarios[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted 377 overlapping labels\n"
     ]
    }
   ],
   "source": [
    "# remove overlapping scenarios, retaining longest label\n",
    "\n",
    "def find_longest_overlap(label1, label2):\n",
    "    span1 = set(range(label1[0], label1[1]))\n",
    "    span2 = set(range(label2[0], label2[1]))\n",
    "\n",
    "    if len(span1.intersection(span2)) == 0:\n",
    "        return 0\n",
    "    elif len(span1) > len(span2):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# find the label indices that are shortest and overlapping\n",
    "deleted_labels = []\n",
    "for scenario in scenarios:\n",
    "    labels = scenario['labels']\n",
    "    overlaps = set()\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(i + 1, len(labels)):\n",
    "            overlap = find_longest_overlap(labels[i], labels[j])\n",
    "            if overlap == 0:\n",
    "                continue\n",
    "            elif overlap < 0:\n",
    "                overlaps.add(j)\n",
    "            else:\n",
    "                overlaps.add(i)\n",
    "    \n",
    "    # delete short overlapping labels from the list\n",
    "    for i in sorted(overlaps, reverse=True):\n",
    "        deleted_labels.append(labels[i][2])\n",
    "        del labels[i]\n",
    "        \n",
    "print('Deleted %i overlapping labels' % len(deleted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, nltk\n",
    "\n",
    "# load the spacey nlp processor for pos-tagging, sentence segmentation\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_label(token, labels):\n",
    "    start = token.idx\n",
    "    end = token.idx + len(token.text)\n",
    "    for label in labels:\n",
    "        if label[0] <= start and end <= label[1]:\n",
    "            return label[2]\n",
    "    return 'O'\n",
    "    \n",
    "tokenized = []\n",
    "for scenario in scenarios:\n",
    "    labels = scenario['labels']\n",
    "    \n",
    "    # parse scenario text and tokenize using the BIO-format\n",
    "    doc = nlp(scenario['text'])\n",
    "    tokenized.append(scenario.copy())\n",
    "    tokenized[-1]['tokens'] = []\n",
    "    last_label = 'O'\n",
    "    sent = []\n",
    "    for token in doc:\n",
    "        if token.is_sent_start:\n",
    "            sent = []\n",
    "            tokenized[-1]['tokens'].append(sent)\n",
    "                      \n",
    "        new_label = get_label(token, labels)\n",
    "        if new_label != 'O':\n",
    "            if new_label != last_label:\n",
    "                last_label = new_label\n",
    "                new_label = 'B-' + new_label\n",
    "            else:\n",
    "                new_label = 'I-' + new_label\n",
    "        else:\n",
    "            last_label = new_label\n",
    "\n",
    "        sent.append([token.text, token.tag_, new_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 300 scenarios and 2664 sentences.\n"
     ]
    }
   ],
   "source": [
    "print('Converted %i scenarios and %i sentences.' % (len(tokenized), sum([len(d['tokens']) for d in tokenized])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset TRAIN: 240 scenarios (0.090), 44623 tokens (0.796):\n",
      "     B-COM: 242 (0.796)\n",
      "     B-QUE: 248 (0.732)\n",
      "     B-SIM: 2808 (0.796)\n",
      "     I-COM: 1362 (0.799)\n",
      "     I-QUE: 1545 (0.741)\n",
      "     I-SIM: 776 (0.792)\n",
      "         O: 37642 (0.799)\n",
      "\n",
      "Dataset TEST: 30 scenarios (0.011), 5761 tokens (0.103):\n",
      "     B-COM: 32 (0.105)\n",
      "     B-QUE: 47 (0.139)\n",
      "     B-SIM: 390 (0.111)\n",
      "     I-COM: 189 (0.111)\n",
      "     I-QUE: 257 (0.123)\n",
      "     I-SIM: 98 (0.100)\n",
      "         O: 4748 (0.101)\n",
      "\n",
      "Dataset VALIDATION: 30 scenarios (0.011), 5647 tokens (0.101):\n",
      "     B-COM: 30 (0.099)\n",
      "     B-QUE: 44 (0.130)\n",
      "     B-SIM: 331 (0.094)\n",
      "     I-COM: 153 (0.090)\n",
      "     I-QUE: 282 (0.135)\n",
      "     I-SIM: 106 (0.108)\n",
      "         O: 4701 (0.100)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "def report_frequencies(dataset):\n",
    "    counter = {dataset_name:Counter() for dataset_name in dataset.keys()}\n",
    "\n",
    "    # report frequences per dataset\n",
    "    total_sent = 0\n",
    "    total_token = 0\n",
    "    total_counter = Counter()\n",
    "    for dataset_name, data in dataset.items():\n",
    "        for scenario in data:\n",
    "            for tokens in scenario['tokens']:\n",
    "                total_sent += 1\n",
    "                for word, pos, tag in tokens:\n",
    "                    counter[dataset_name][tag] += 1\n",
    "                    total_counter[tag] += 1\n",
    "                    total_token += 1\n",
    "\n",
    "    for dataset_name, data in dataset.items():\n",
    "        token_count = sum([c for c in counter[dataset_name].values()])\n",
    "        print('\\nDataset %s: %i scenarios (%0.3f), %i tokens (%0.3f):' % (\n",
    "            dataset_name.upper(),\n",
    "            len(data),\n",
    "            len(data) / total_sent,\n",
    "            token_count,\n",
    "            token_count / total_token\n",
    "        ))\n",
    "        labels = sorted(list(counter[dataset_name].keys()))\n",
    "        for label in labels:\n",
    "            print('%s: %i (%0.3f)' % (\n",
    "                label.rjust(10), \n",
    "                counter[dataset_name][label], \n",
    "                counter[dataset_name][label] / \n",
    "                total_counter[label]\n",
    "            ))\n",
    "\n",
    "dataset = {}\n",
    "if False:\n",
    "    total_scenarios = len(tokenized)\n",
    "    index1 = int(total_scenarios * 0.80)\n",
    "    index2 = int(index1 + (total_scenarios * 0.10))\n",
    "\n",
    "    # randomize the scenarios by scenario_id\n",
    "    shuffled = tokenized.copy()\n",
    "    random.shuffle(shuffled)\n",
    "    \n",
    "    # divide randomized sentences into train, test, validation splits\n",
    "    dataset = {\n",
    "        'train': shuffled[0:index1],\n",
    "        'test': shuffled[index1:index2],\n",
    "        'validation': shuffled[index2:],\n",
    "    }\n",
    "else:\n",
    "    split_ids = json.load(open('dataset_split_ids.json'))\n",
    "    dataset = {'train': [], 'test': [], 'validation': []}\n",
    "    for scenario in tokenized:\n",
    "        if scenario['scenario_id'] in split_ids['train_ids']:\n",
    "            dataset['train'].append(scenario)\n",
    "        elif scenario['scenario_id'] in split_ids['test_ids']:\n",
    "            dataset['test'].append(scenario)\n",
    "        elif scenario['scenario_id'] in split_ids['validation_ids']:\n",
    "            dataset['validation'].append(scenario)\n",
    "\n",
    "report_frequencies(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(dataset, open('../datasets/scenarios-training.json', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
