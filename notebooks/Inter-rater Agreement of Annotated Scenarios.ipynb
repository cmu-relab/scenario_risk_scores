{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze scenarios from two raters\n",
    "\n",
    "This notebook parses scenarios annotated using a pre-agreed coding frame to compute the inter-rater agreement Kappa statistic for above-chance agreement, and to review agreements and disagreements.\n",
    "\n",
    "The notebook also creates a file that includes the labeled words to identify code mismatches between the two raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from lib_analysis import read_raw_sample\n",
    "\n",
    "read_raw_sample(\"../datasets/scenarios1.json\", 10, \"../datasets/sample3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib_analysis import read_and_parse_data, is_consistent\n",
    "from lib_analysis import read_data\n",
    "\n",
    "\n",
    "# data1 = read_and_parse_data('../datasets/sample1-TH.json')\n",
    "# data2 = read_and_parse_data('../datasets/sample1-vk-reann.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "# extract all square brackets and write to excel\n",
    "import re\n",
    "# print(data1['MAS-G-0027'])\n",
    "# print(type(data1))\n",
    "# data1 = read_and_parse_data('../datasets/sample1-TH.json')\n",
    "# data2 = read_and_parse_data('../datasets/sample2-TH.txt')\n",
    "# data3 = read_and_parse_data('../datasets/sample3-TH.txt')\n",
    "# data = data1 | data2 | data3\n",
    "data = read_and_parse_data('../datasets/scenarios2_ex-TH.txt')\n",
    "print(len(data))\n",
    "phrases = []\n",
    "for key, value in data.items():\n",
    "    text = value['text']\n",
    "    # find and extract all patterns in square brackets\n",
    "    # phrases.append(re.findall(r'\\[(.*?)\\]', text))\n",
    "    phrases += re.findall(r'\\[(.*?)\\]', text)\n",
    "# write to excel\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(phrases)\n",
    "df.to_excel('scenario2.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'MAS-G-0044', 'MAS-G-0065', 'MAS-G-0028', 'MAS-G-0030', 'MAS-G-0058', 'MAS-G-0088', 'MAS-G-0061', 'MAS-G-0071', 'MAS-G-0076', 'MAS-G-0009', 'MAS-G-0066', 'MAS-G-0003', 'MAS-G-0077', 'MAS-G-0015', 'MAS-G-0100', 'MAS-G-0073', 'MAS-G-0037', 'MAS-G-0007', 'MAS-G-0072', 'MAS-G-0019', 'MAS-G-0074', 'MAS-G-0098', 'MAS-G-0033', 'MAS-G-0090', 'MAS-G-0020', 'MAS-G-0081', 'MAS-G-0027', 'MAS-G-0021', 'MAS-G-0096', 'MAS-G-0086'}\n"
     ]
    }
   ],
   "source": [
    "# s1, s2, s3 = read_data(\"../datasets/sample1-TH.json\"), read_data(\"../datasets/sample2-TH.txt\"), read_data(\"../datasets/sample3-TH.txt\")\n",
    "# sample = []\n",
    "# for keys in [s1.keys(), s2.keys(), s3.keys()]:\n",
    "#     for k in keys:\n",
    "#         sample.append(k)\n",
    "# sample = set(sample)\n",
    "# print(sample)\n",
    "\n",
    "# import random, json, os\n",
    "# def split_sample(filename, sample_size, dest, exclude_list):\n",
    "#     data = \"\"\n",
    "#     f = open(filename, 'r')\n",
    "#     sample_list = json.load(f)\n",
    "#     for sample in sample_list:\n",
    "#         if sample['scenario_id'] not in exclude_list:\n",
    "#             data += sample[\"app_url\"] + \"\\n\" + sample['scenario_id'] + \": \" + sample[\"text\"] + \"\\n\\n\"\n",
    "#     if not os.path.exists(dest):\n",
    "#         with open(dest, 'w') as f:\n",
    "#             f.write(data)\n",
    "\n",
    "# split_sample(\"../datasets/scenarios1.json\", 100, \"../datasets/scenarios1_ex.txt\", sample)\n",
    "# split_sample(\"../datasets/scenarios2.json\", 50, \"../datasets/scenarios2_ex.txt\", sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario IDs matched.\n"
     ]
    }
   ],
   "source": [
    "is_consistent(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa, All Codes: 0.7091\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import csv\n",
    "\n",
    "scenario_ids = list(data1.keys())\n",
    "all_codes1 = [c for d in data1.values() for c in d['codes']]\n",
    "all_codes2 = [c for d in data2.values() for c in d['codes']]\n",
    "\n",
    "# uncomment to compute kappa on non-BIO code format\n",
    "#all_codes1 = ['o' if len(c) == 1 else c[2:] for d in data1.values() for c in d['codes']]\n",
    "#all_codes2 = ['o' if len(c) == 1 else c[2:] for d in data2.values() for c in d['codes']]\n",
    "\n",
    "kappa = cohen_kappa_score(all_codes1, all_codes2)\n",
    "print('Cohen\\'s Kappa, All Codes: %0.4f' % kappa)\n",
    "\n",
    "# write the words and simplified codes for both datasets\n",
    "# simplified codes: the b/i prefixes are removed\n",
    "with open('coded_data.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['word','rater1','rater2'])\n",
    "    for scenario_id in scenario_ids:\n",
    "        words = data1[scenario_id]['words']\n",
    "        codes1 = ['o' if len(c) == 1 else c[2:] for c in data1[scenario_id]['codes']]\n",
    "        codes2 = ['o' if len(c) == 1 else c[2:] for c in data2[scenario_id]['codes']]\n",
    "        for i in range(len(words)):\n",
    "            writer.writerow([words[i], codes1[i], codes2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count code distributions\n",
    "possible_codes = sorted(list(set(all_codes1).union(set(all_codes2))))\n",
    "tally = {'r1': {c:0 for c in possible_codes},\n",
    "        'r2': {c:0 for c in possible_codes}}\n",
    "for c in all_codes1:\n",
    "    tally['r1'][c] += 1\n",
    "for c in all_codes2:\n",
    "    tally['r2'][c] += 1\n",
    "print('\\tRater1\\tRater2')\n",
    "for c in possible_codes:\n",
    "    print('%s\\t%s\\t%s' % (c, tally['r1'][c], tally['r2'][c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa, Flow, Only: 0.6086\n"
     ]
    }
   ],
   "source": [
    "# use simplified codes\n",
    "flow_only1 = ['o' if len(c) == 1 else c[2:] for d in data1.values() for c in d['codes']]\n",
    "flow_only1 = [c if c != 'i' else 'o' for c in all_codes1]\n",
    "\n",
    "flow_only2 = ['o' if len(c) == 1 else c[2:] for d in data2.values() for c in d['codes']]\n",
    "flow_only2 = [c if c != 'i' else 'o' for c in all_codes2]\n",
    "\n",
    "kappa = cohen_kappa_score(flow_only1, flow_only2)\n",
    "print('Cohen\\'s Kappa, Flow, Only: %0.4f' % kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index information types into tuples: i, j, score, phrase                      \n",
    "def index_infotype(data):\n",
    "    info = []\n",
    "    phrase = []\n",
    "    j = -1\n",
    "    for i, (word, code) in enumerate(zip(data['words'], data['codes'])):\n",
    "        if code == 'b-i':\n",
    "            phrase = [word]\n",
    "            j = i\n",
    "        elif code == 'i-i':\n",
    "            phrase.append(word)\n",
    "        elif code == 'o' and j >= 0:\n",
    "            info.append((j, j+len(phrase), data['scores'][len(info)], ' '.join(phrase)))\n",
    "            phrase = []\n",
    "            j = -1\n",
    "    return info\n",
    "\n",
    "# identify risk scores for overlapping information types\n",
    "def overlaps(i1, j1, i2, j2):\n",
    "    return len(set(range(i1, j1)).intersection(set(range(i2, j2)))) > 0\n",
    "\n",
    "def find_overlaps(info1, info2):\n",
    "    overlap = []\n",
    "    for i1, j1, score1, phrase1 in info1:\n",
    "        for i2, j2, score2, phrase2 in info2:\n",
    "            if overlaps(i1, j1, i2, j2):\n",
    "                overlap.append([(score1, phrase1), (score2, phrase2)])\n",
    "    return overlap\n",
    "\n",
    "agreed = 0\n",
    "disagreed = 0\n",
    "for scenario_id in data1.keys():\n",
    "    info1 = index_infotype(data1[scenario_id])\n",
    "    info2 = index_infotype(data2[scenario_id])\n",
    "    overlap = find_overlaps(info1, info2)\n",
    "\n",
    "    for i, ((s1, p1), (s2, p2)) in enumerate(overlap):\n",
    "        print('\\n%s, match %i: score %i, %s' % (scenario_id, i, int(s1), p1))\n",
    "        print('%s, match %i: score %i, %s' % (scenario_id, i, int(s2), p2))\n",
    "        \n",
    "    agreed += len(overlap)\n",
    "    disagreed += len(info1) - len(overlap) + len(info2) - len(overlap)\n",
    "\n",
    "print('\\nAgreed: %i' % agreed)\n",
    "print('Disagreed: %i' % disagreed)\n",
    "\n",
    "scores1 = [int(s) for d in data1.values() for s in d['scores']]\n",
    "scores2 = [int(s) for d in data2.values() for s in d['scores']]  \n",
    "print('\\nScore average for Rater 1: %0.4f' % (sum(scores1) / len(scores1)))\n",
    "print('Score average for Rater 2: %0.4f' % (sum(scores2) / len(scores2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the disagreements out to a file for inspection\n",
    "disagreed = []\n",
    "\n",
    "# format of data1/data2: sent_id, word_id, code\n",
    "for x1, x2 in zip(data1, data2):\n",
    "    if x1[2] != x2[2]:\n",
    "        # record the sent_id, word_id, word, codes, plus the sentence\n",
    "        sent = list(sentences[x1[0]])\n",
    "        sent[x1[1]] = '[' + sent[x1[1]] + ']'\n",
    "        disagreed.append([\n",
    "            x1[0], x1[1], x1[2], x2[2], ' '.join(sent)\n",
    "        ])\n",
    "\n",
    "with open('disagreements.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['sent_id', 'word_id', 'code1', 'code2', 'sentence'])\n",
    "    for row in disagreed:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e084cb2b20e6baf74d084ddc77ed145dd17f40d40600394db9131686a59b1288"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
