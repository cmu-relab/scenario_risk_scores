{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze scenarios from two raters\n",
    "\n",
    "This notebook parses scenarios annotated using a pre-agreed coding frame to compute the inter-rater agreement Kappa statistic for above-chance agreement, and to review agreements and disagreements.\n",
    "\n",
    "The notebook also creates a file that includes the labeled words to identify code mismatches between the two raters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in d:\\install\\anaconda\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: jinja2 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (21.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (4.62.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (1.20.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (2.26.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (8.1.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: setuptools in d:\\install\\anaconda\\lib\\site-packages (from spacy) (58.0.4)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in d:\\install\\anaconda\\lib\\site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in d:\\install\\anaconda\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in d:\\install\\anaconda\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\install\\anaconda\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy) (3.10.0.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\install\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\install\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\install\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\install\\anaconda\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in d:\\install\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.1)\n",
      "Requirement already satisfied: blis<0.10.0,>=0.7.8 in d:\\install\\anaconda\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.9.1)\n",
      "Requirement already satisfied: colorama in d:\\install\\anaconda\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in d:\\install\\anaconda\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in d:\\install\\anaconda\\lib\\site-packages (from jinja2->spacy) (1.1.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "âœ” Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "%pip install spacy\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from lib_analysis import read_raw_sample\n",
    "\n",
    "read_raw_sample(\"../datasets/scenarios1.json\", 10, \"../datasets/sample3.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib_analysis import read_and_parse_data, is_consistent\n",
    "from lib_analysis import read_data\n",
    "\n",
    "\n",
    "data1 = read_and_parse_data('../datasets/sample2-TH.txt')\n",
    "data2 = read_and_parse_data('../datasets/sample2-vk.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scenario IDs matched.\n"
     ]
    }
   ],
   "source": [
    "is_consistent(data1, data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa, All Codes: 0.5742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import cohen_kappa_score\n",
    "import csv\n",
    "\n",
    "scenario_ids = list(data1.keys())\n",
    "all_codes1 = [c for d in data1.values() for c in d['codes']]\n",
    "all_codes2 = [c for d in data2.values() for c in d['codes']]\n",
    "\n",
    "# uncomment to compute kappa on non-BIO code format\n",
    "#all_codes1 = ['o' if len(c) == 1 else c[2:] for d in data1.values() for c in d['codes']]\n",
    "#all_codes2 = ['o' if len(c) == 1 else c[2:] for d in data2.values() for c in d['codes']]\n",
    "\n",
    "kappa = cohen_kappa_score(all_codes1, all_codes2)\n",
    "print('Cohen\\'s Kappa, All Codes: %0.4f' % kappa)\n",
    "\n",
    "# write the words and simplified codes for both datasets\n",
    "# simplified codes: the b/i prefixes are removed\n",
    "with open('coded_data.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['word','rater1','rater2'])\n",
    "    for scenario_id in scenario_ids:\n",
    "        words = data1[scenario_id]['words']\n",
    "        codes1 = ['o' if len(c) == 1 else c[2:] for c in data1[scenario_id]['codes']]\n",
    "        codes2 = ['o' if len(c) == 1 else c[2:] for c in data2[scenario_id]['codes']]\n",
    "        for i in range(len(words)):\n",
    "            writer.writerow([words[i], codes1[i], codes2[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tRater1\tRater2\n",
      "b-i\t91\t100\n",
      "i-i\t303\t219\n",
      "o\t1666\t1741\n"
     ]
    }
   ],
   "source": [
    "# count code distributions\n",
    "possible_codes = sorted(list(set(all_codes1).union(set(all_codes2))))\n",
    "tally = {'r1': {c:0 for c in possible_codes},\n",
    "        'r2': {c:0 for c in possible_codes}}\n",
    "for c in all_codes1:\n",
    "    tally['r1'][c] += 1\n",
    "for c in all_codes2:\n",
    "    tally['r2'][c] += 1\n",
    "print('\\tRater1\\tRater2')\n",
    "for c in possible_codes:\n",
    "    print('%s\\t%s\\t%s' % (c, tally['r1'][c], tally['r2'][c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa, Flow, Only: 0.5742\n"
     ]
    }
   ],
   "source": [
    "# use simplified codes\n",
    "flow_only1 = ['o' if len(c) == 1 else c[2:] for d in data1.values() for c in d['codes']]\n",
    "flow_only1 = [c if c != 'i' else 'o' for c in all_codes1]\n",
    "\n",
    "flow_only2 = ['o' if len(c) == 1 else c[2:] for d in data2.values() for c in d['codes']]\n",
    "flow_only2 = [c if c != 'i' else 'o' for c in all_codes2]\n",
    "\n",
    "kappa = cohen_kappa_score(flow_only1, flow_only2)\n",
    "print('Cohen\\'s Kappa, Flow, Only: %0.4f' % kappa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index information types into tuples: i, j, score, phrase                      \n",
    "def index_infotype(data):\n",
    "    info = []\n",
    "    phrase = []\n",
    "    j = -1\n",
    "    for i, (word, code) in enumerate(zip(data['words'], data['codes'])):\n",
    "        if code == 'b-i':\n",
    "            phrase = [word]\n",
    "            j = i\n",
    "        elif code == 'i-i':\n",
    "            phrase.append(word)\n",
    "        elif code == 'o' and j >= 0:\n",
    "            info.append((j, j+len(phrase), data['scores'][len(info)], ' '.join(phrase)))\n",
    "            phrase = []\n",
    "            j = -1\n",
    "    return info\n",
    "\n",
    "# identify risk scores for overlapping information types\n",
    "def overlaps(i1, j1, i2, j2):\n",
    "    return len(set(range(i1, j1)).intersection(set(range(i2, j2)))) > 0\n",
    "\n",
    "def find_overlaps(info1, info2):\n",
    "    overlap = []\n",
    "    for i1, j1, score1, phrase1 in info1:\n",
    "        for i2, j2, score2, phrase2 in info2:\n",
    "            if overlaps(i1, j1, i2, j2):\n",
    "                overlap.append([(score1, phrase1), (score2, phrase2)])\n",
    "    return overlap\n",
    "\n",
    "agreed = 0\n",
    "disagreed = 0\n",
    "for scenario_id in data1.keys():\n",
    "    info1 = index_infotype(data1[scenario_id])\n",
    "    info2 = index_infotype(data2[scenario_id])\n",
    "    overlap = find_overlaps(info1, info2)\n",
    "\n",
    "    for i, ((s1, p1), (s2, p2)) in enumerate(overlap):\n",
    "        print('\\n%s, match %i: score %i, %s' % (scenario_id, i, int(s1), p1))\n",
    "        print('%s, match %i: score %i, %s' % (scenario_id, i, int(s2), p2))\n",
    "        \n",
    "    agreed += len(overlap)\n",
    "    disagreed += len(info1) - len(overlap) + len(info2) - len(overlap)\n",
    "\n",
    "print('\\nAgreed: %i' % agreed)\n",
    "print('Disagreed: %i' % disagreed)\n",
    "\n",
    "scores1 = [int(s) for d in data1.values() for s in d['scores']]\n",
    "scores2 = [int(s) for d in data2.values() for s in d['scores']]  \n",
    "print('\\nScore average for Rater 1: %0.4f' % (sum(scores1) / len(scores1)))\n",
    "print('Score average for Rater 2: %0.4f' % (sum(scores2) / len(scores2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the disagreements out to a file for inspection\n",
    "disagreed = []\n",
    "\n",
    "# format of data1/data2: sent_id, word_id, code\n",
    "for x1, x2 in zip(data1, data2):\n",
    "    if x1[2] != x2[2]:\n",
    "        # record the sent_id, word_id, word, codes, plus the sentence\n",
    "        sent = list(sentences[x1[0]])\n",
    "        sent[x1[1]] = '[' + sent[x1[1]] + ']'\n",
    "        disagreed.append([\n",
    "            x1[0], x1[1], x1[2], x2[2], ' '.join(sent)\n",
    "        ])\n",
    "\n",
    "with open('disagreements.csv', 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['sent_id', 'word_id', 'code1', 'code2', 'sentence'])\n",
    "    for row in disagreed:\n",
    "        writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e084cb2b20e6baf74d084ddc77ed145dd17f40d40600394db9131686a59b1288"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
